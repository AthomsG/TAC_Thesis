2 Nov 2023

- Will test implemented value networks on random input tensors and on Gym environment;
- The networks take as input random tensors with shape=(BATCH_SIZE, 4, 84, 84). Noticed that CNN input tensor shape is expected to be different in PyTorch and Tensorflow. Keep this in mind for the future;
- Tested Actor network for all 3 different alpha values
- Before testing model on gym environment observations, I have to preprocess these first.

4 Nov 2023

- Used Xiang Li's implementation of atari_env to process (wrap) the models environments. Uses OpenAI Gym's library;
- Tested Actor & Critic networks on environments output states;
- Started implementing RAC agent class. It is the class where the regularized actor critic algorithm for the discrete scenario is implemented. (get action, save and load models weights methods. All have been tested and work)

5 Nov 2023

- Continuing to implementation of the RAC class.
- Implemented a proto gradient_step method. Had to troubleshoot a computational graph issue with solution [here](https://discuss.pytorch.org/t/multiple-loss-functions-in-a-model/111464/14).

- PROBLEM SOURCE:

'''
self.critic1_opt.zero_grad()
v_loss.backward()
self.critic1_opt.step()

self.actor_opt.zero_grad()
p_loss.backward()
self.actor_opt.step()
'''

- PROBLEM SOLUTION: (It had to do with the fact that I'm sharing tensors in the different losses)

'''
self.critic1_opt.zero_grad()
self.actor_opt.zero_grad()

v_loss.backward(retain_graph=True)
p_loss.backward()

self.critic1_opt.step()
self.actor_opt.step()
'''

6 Nov 2023

- Changed VisualizeEnv class to output states with the buffer size included in the first index (avoid having to reshape frequently);
- Organised code by separating Actor and Critic classes into separate .py file;
- Implemented a version of the full model. Used same hyperparameters as Xiang Li but model didn't takeoff (on Breakout!). I believe this has to do with the fact I'm not running through the whole dataset, only one batch. 
- Apparently this is not really an issue. We're not supposed to perform the gradient step on the whole replay buffer.

18 Nov 2023

- Tested PyTorch implementation on the testing environment where observation are uniform random with +1 reward if action==0 (or any other number) and 0 if not. Algorithm learns the optimal policy.
- Checking whether the issue is due to the fact that the environment is being reset when done==1, as it doesn't seem to be explicit on tensorflow code.
- Trying to understand where the env.reset() is being called, because I can't find it! -> It seems that env.reset() is being called on agent env.step(), whenever done==1, but not yet sure. Maybe it has to do with the fact that we're using agent on train_debug.py?
- Started recording frames during environment steps. Board is leaning on one of the two corners and isn't moving much. It doesn't seem like takeoff is happening, but maybe it has to do with the seeds that are being used? Shannon entropy was reported to be very robust.
- Both environments share the same version of GYM
- Checked if it might have something to do with how the transitions are being stored in the replay buffer (experiment in test_code).
- Noticed that RQN implementation in PyTorch has ReLu activation on the final layer of the critic network. Tensorflow code has no activation function on the last layer of the critic network. Will change this and leave code running. -> Episodes where longer but rewards were fewer.

19 Nov 2023

- Will compare value and policy losses with tensorflow model. -> loss values are completely different as the policy is positive with magnitude ~1e3 and tensorflow has negative ~1e-1. Maybe the policy loss is not being properly computed (equation 10). I think something suspicious is happening with the variable 'action_policy' in the compute_gradient method. I still haven't checked it, but maybe it also has to do with the environment? because in my pytorch implementation I'm not using agent class...
- Created new Gridworld Test environment, where agent starts in the center of the grid and has ending point on the 4 corners.

21 Nov 2023

- Implemented simplified version of the algorithm (2 MLP instead of CNN network) to check algorithm. I think I found the error. equation 10 was poorly implemented:

 Instead of using action_(...) -> p_loss = (policy * (- self.lambd * phi - qvals)).sum()/self.batch_size

22 Nov 2023

- Turns out equation 10 was computing the gradient for both network parameters, instead of computing only for Actor network. This problem has been fixed with replacing: qvals -> qvals.detatch()
- Still need to check if code is running as the Tensorflow implementation.

Experiment Parameters:

	- lambda: 0.1
	- regularization: Shannon
	- batch_size: 32
	- update_target_every: 2500
	- memory_size: 100000
	- num_iterations: 200000
	- learning rate: 1e-4
	- discount factor: 0.99
	- env: BreakoutNoFrameskip-v4
	- learning_starts: 5000

23 Nov 2023

- Results don't seem to be quite there yet. Will now run the experiment with a breakpoint gradient step by gradient step to compare results and assess what seem to be the main differences and take it fro there. p_loss seem to have the same order of magnitude initially, but v_loss is ~10 times smaller. There's a problem with the dones batch on the PyTorch implementation. Maybe the problem is in the environment class implementation. 
- After fixing dones not counting correctly, noticed that Critic network in both frameworks aren't outputting similar values. Maybe this has to do with weight initialisation? I think it was the case. That and the fact that it seems that v_loss in PyTorch is not being divided by batch size while in tensorflow that is the case.

24 Nov 2024

- There are still observable differences between implementations, namely on the policy and value loss functions. Will check on tensor value again, using breakpoints.
- Seeds on the Tensorflow aren't producing exactly the same results!
- Tomorrow I'll initialise all networks with the same weights.

25 Nov 2025

- Test code to initialize networks with the same weights. I believe this will beuseful for troubleshooting.