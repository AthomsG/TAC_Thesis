2 Nov 2023

- Will test implemented value networks on random input tensors and on Gym environment;
- The networks take as input random tensors with shape=(BATCH_SIZE, 4, 84, 84). Noticed that CNN input tensor shape is expected to be different in PyTorch and Tensorflow. Keep this in mind for the future;
- Tested Actor network for all 3 different alpha values
- Before testing model on gym environment observations, I have to preprocess these first.

4 Nov 2023

- Used Xiang Li's implementation of atari_env to process (wrap) the models environments. Uses OpenAI Gym's library;
- Tested Actor & Critic networks on environments output states;
- Started implementing RAC agent class. It is the class where the regularized actor critic algorithm for the discrete scenario is implemented. (get action, save and load models weights methods. All have been tested and work)

5 Nov 2023

- Continuing to implementation of the RAC class.
- Implemented a proto gradient_step method. Had to troubleshoot a computational graph issue with solution [here](https://discuss.pytorch.org/t/multiple-loss-functions-in-a-model/111464/14).

- PROBLEM SOURCE:

'''
self.critic1_opt.zero_grad()
v_loss.backward()
self.critic1_opt.step()

self.actor_opt.zero_grad()
p_loss.backward()
self.actor_opt.step()
'''

- PROBLEM SOLUTION: (It had to do with the fact that I'm sharing tensors in the different losses)

'''
self.critic1_opt.zero_grad()
self.actor_opt.zero_grad()

v_loss.backward(retain_graph=True)
p_loss.backward()

self.critic1_opt.step()
self.actor_opt.step()
'''

6 Nov 2023

- Changed VisualizeEnv class to output states with the buffer size included in the first index (avoid having to reshape frequently);
- Organised code by separating Actor and Critic classes into separate .py file;
- Implemented a version of the full model. Used same hyperparameters as Xiang Li but model didn't takeoff (on Breakout!). I believe this has to do with the fact I'm not running through the whole dataset, only one batch. 
- Apparently this is not really an issue. We're not supposed to perform the gradient step on the whole replay buffer.

18 Nov 2023

- Tested PyTorch implementation on the testing environment where observation are uniform random with +1 reward if action==0 (or any other number) and 0 if not. Algorithm learns the optimal policy.
- Checking whether the issue is due to the fact that the environment is being reset when done==1, as it doesn't seem to be explicit on tensorflow code.
- Trying to understand where the env.reset() is being called, because I can't find it! -> It seems that env.reset() is being called on agent env.step(), whenever done==1, but not yet sure. Maybe it has to do with the fact that we're using agent on train_debug.py?
- Started recording frames during environment steps. Board is leaning on one of the two corners and isn't moving much. It doesn't seem like takeoff is happening, but maybe it has to do with the seeds that are being used? Shannon entropy was reported to be very robust.
- Both environments share the same version of GYM
- Checked if it might have something to do with how the transitions are being stored in the replay buffer (experiment in test_code).
- Noticed that RQN implementation in PyTorch has ReLu activation on the final layer of the critic network. Tensorflow code has no activation function on the last layer of the critic network. Will change this and leave code running. -> Episodes where longer but rewards were fewer.

19 Nov 2023

- Will compare value and policy losses with tensorflow model. -> loss values are completely different as the policy is positive with magnitude ~1e3 and tensorflow has negative ~1e-1. Maybe the policy loss is not being properly computed (equation 10). I think something suspicious is happening with the variable 'action_policy' in the compute_gradient method. I still haven't checked it, but maybe it also has to do with the environment? because in my pytorch implementation I'm not using agent class...
- Created new Gridworld Test environment, where agent starts in the center of the grid and has ending point on the 4 corners.

21 Nov 2023

- Implemented simplified version of the algorithm (2 MLP instead of CNN network) to check algorithm. I think I found the error. equation 10 was poorly implemented:

 Instead of using action_(...) -> p_loss = (policy * (- self.lambd * phi - qvals)).sum()/self.batch_size

22 Nov 2023

- Turns out equation 10 was computing the gradient for both network parameters, instead of computing only for Actor network. This problem has been fixed with replacing: qvals -> qvals.detatch()
- Still need to check if code is running as the Tensorflow implementation.

Experiment Parameters:

	- lambda: 0.1
	- regularization: Shannon
	- batch_size: 32
	- update_target_every: 2500
	- memory_size: 100000
	- num_iterations: 200000
	- learning rate: 1e-4
	- discount factor: 0.99
	- env: BreakoutNoFrameskip-v4
	- learning_starts: 5000

23 Nov 2023

- Results don't seem to be quite there yet. Will now run the experiment with a breakpoint gradient step by gradient step to compare results and assess what seem to be the main differences and take it fro there. p_loss seem to have the same order of magnitude initially, but v_loss is ~10 times smaller. There's a problem with the dones batch on the PyTorch implementation. Maybe the problem is in the environment class implementation. 
- After fixing dones not counting correctly, noticed that Critic network in both frameworks aren't outputting similar values. Maybe this has to do with weight initialisation? I think it was the case. That and the fact that it seems that v_loss in PyTorch is not being divided by batch size while in tensorflow that is the case.

24 Nov 2023

- There are still observable differences between implementations, namely on the policy and value loss functions. Will check on tensor value again, using breakpoints.
- Seeds on the Tensorflow aren't producing exactly the same results!
- Tomorrow I'll initialise all networks with the same weights.

25 Nov 2023

- Test code to initialize networks with the same weights. I believe this will be useful for troubleshooting.

26 Nov 2023

- Replay buffer implementation is the exact same on both cases. Will check both methods on the same batch to see if it helps finding where the error comes from. Saved replay buffer copy as 'buffer copy.py'.
- Noticed that 'global_step' in Tensorflow is number of gradient steps the agent has taken... maybe this is a source of difference between algorithms.

1 Dec 2023

- So I think I'll focus on sum of rewards and episode length as performance metrics. Will ask Zita and Andr√© if the differences between loss functions can be explained by the Auto Grad libraries or if there's something more 'mysterious' at play...
- PyTorch implementation has moment of 'catastrophic forgetting' at some point (when experiment slows down a lot...). Maybe gradient clipping on the policy will help?

12 Dec 2023

- So as a final test I'll check if there are differences in how the loss functions are computed in both frameworks: Both libraries output the same loss for the same input.
- Ran experiment for the Boxing environment with alpha=2 Tsallis entropy regularization and temperature 0.1 (reported on the paper to have the best results). The optimal policy is a Sparsemax policy.

15 Dec 2023

- Forgot to update the 'phi' values (was using Shannon entropy on the objective function). This change has now been added.
- Updated phi matches natural logarithm. 3 runs of 88k environment steps were performed and results match what is to be expected.

16 Dec 2023

- All phi values were 0 and it had to do with the epsilon value (apparently it was too small).
- I had forgotten to consider that the 'update_target_every' step regards GRADIENT steps and not ENVIRONMENT STEPS. Maybe that's why my algorithm converges faster than its tensorflow counterpart.

17 Dec 2023

- Will check if Tensorflow Actor network architecture yields better results than alpha-entmax transformation.

21 Dec 2023

- Will use polyak averaging instead of hard updates and see if this stabilises reward curves.

2 Jan 2024

- The lenghts aren't the same! When I use Actor network with the same architecture as the Tensorflow implementation (normalized ReLU layers) loss functions seem to be the same. There is something still that is hard to understand. Episode length is not the same across both implementations.

16 Jan 2024

- There was an error in my Pytorch impmlementation of the Tensorflow actor network. This has been corrected! (hopefully). I had forgotten to perform element-wise multiplication of the last 2 layers for the cases alpha != 1.

17 Jan 2024

- Started printing gradient values to spot where the 'NaN' logits come from. I'mm debugging the code in the 'troubleshoot_entmax15' directory. The problem arises in the policy network. For the entmax15, lamb = 0.1, seed 0 we get:

Gradient of policy: tensor([[-0.0080, -0.0061, -0.0059, -0.0078, -0.0048, -0.0049, -0.0083, -0.0068,
         -0.0054, -0.0058, -0.0070, -0.0045, -0.0049, -0.0074,     nan, -0.0078,
         -0.0067, -0.0068], .....

One thing I noticed is that the 'nan' entry in the gradient of the policy coincides with the entry of the action that has probability 0 of occurring. Could this be change or is there something bigger at play here? Will check if the error only happens when some policy entries are 0.
- The problem arises whenever the policy is sparse. Will now look for the calculation that outputs a 'nan'.

18 Jan 2024

- I will check if the 'nan' gradients arise from a negative value close to 0 in the policy. For example:

In [9]: policy = torch.tensor([-0.0000000001])

In [10]: - (policy**(1.5 - 1) - 1)/(1.5 * (0.5))
Out[10]: tensor([nan])

Maybe this can be the case, but I'm still not sure.

24 Jan 2024

- Tried clamping of the policy: policy = policy.clamp(min=1e-10).
This helped with not having problems, however the algorithm doesn't converge.
- Created gradient_setp_by_step.py to store code to paste on debug command line when the algorithm goes to shit. All intermediate values are being stored in an individual '.txt' inside 'debug_prints' directory.
- Ok, I think I finally understand the problem at hand. The gradient of the policy loss (see: https://www.overleaf.com/project/651ddbcc74d7b212b0302ef7) has a term proportional to 1/policy, which isn't defined for sparse policies.

28 Jan 2024

- Now that I think if figured the issue with the sparse policies, I'll try to reproduce the results from 'a reguralized approach to sparse optimal policies', by changing the q-log to its particular case, to avoid non defined gradient issue. KEEP IN MIND ABOUT THE EXPERIMENT! I changed the debug variable to false, removed the IPython embed whenever the actor produces a sparse policy during gradient steps.

3 Fev 2024

- So actually, there's no theoretical problem with the loss function. I'm just going to use the code that works, from the SAC_discrete repo.

6 Fev 2024

- I've implemented the algorithm from scratch for a trivial environment, computing everything analytically. The results seem reasonable. The algorithm learns. However, for the same environment, the pytorch implementation doesn't seem to be learning. The critic loss results match, however the policy loss values don't! I believe the error is in how the policy loss is implemented.